{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried LSA and NMF with count vectorizer and tf-idf, decided on nmf with count vectorizer and four topics. Used that in bush_graphs to make graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('clean_abstract_bush.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peter marks analysis finds that commercials ru...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the new york times the internet and political ...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>letter by mike fremont of rivers unlimited on ...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidential primary season is the most compet...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>editorial on various campaign proposals for us...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40816</th>\n",
       "      <td>republicans have criticized her tweets but dem...</td>\n",
       "      <td>2021-02-25 20:56:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40817</th>\n",
       "      <td>the disputes are reminiscent of the fight surr...</td>\n",
       "      <td>2021-02-26 00:12:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40818</th>\n",
       "      <td>most presidents leave the white house and adop...</td>\n",
       "      <td>2021-02-27 17:00:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40819</th>\n",
       "      <td>democracy an unassuming policy journal with an...</td>\n",
       "      <td>2021-02-28 22:02:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40820</th>\n",
       "      <td>despite falling from power in washington the r...</td>\n",
       "      <td>2021-03-01 22:43:35+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40821 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      peter marks analysis finds that commercials ru...   \n",
       "1      the new york times the internet and political ...   \n",
       "2      letter by mike fremont of rivers unlimited on ...   \n",
       "3      presidential primary season is the most compet...   \n",
       "4      editorial on various campaign proposals for us...   \n",
       "...                                                  ...   \n",
       "40816  republicans have criticized her tweets but dem...   \n",
       "40817  the disputes are reminiscent of the fight surr...   \n",
       "40818  most presidents leave the white house and adop...   \n",
       "40819  democracy an unassuming policy journal with an...   \n",
       "40820  despite falling from power in washington the r...   \n",
       "\n",
       "                            date  \n",
       "0      2000-01-01 05:00:00+00:00  \n",
       "1      2000-01-01 05:00:00+00:00  \n",
       "2      2000-01-02 05:00:00+00:00  \n",
       "3      2000-01-02 05:00:00+00:00  \n",
       "4      2000-01-02 05:00:00+00:00  \n",
       "...                          ...  \n",
       "40816  2021-02-25 20:56:39+00:00  \n",
       "40817  2021-02-26 00:12:38+00:00  \n",
       "40818  2021-02-27 17:00:07+00:00  \n",
       "40819  2021-02-28 22:02:50+00:00  \n",
       "40820  2021-03-01 22:43:35+00:00  \n",
       "\n",
       "[40821 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend(['could','many','even','also','make','whether','least','called','keep','said','says', 'say',\n",
    "                      'george','bush',' w ','bushs','mr','pres','would','president','sen','us','united', 'states','american','americans',\n",
    "                       'white','house','new','york','way','people','year','years','sec','state','photo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'could',\n",
       " 'many',\n",
       " 'even',\n",
       " 'also',\n",
       " 'make',\n",
       " 'whether',\n",
       " 'least',\n",
       " 'called',\n",
       " 'keep',\n",
       " 'said',\n",
       " 'says',\n",
       " 'say',\n",
       " 'george',\n",
       " 'bush',\n",
       " ' w ',\n",
       " 'bushs',\n",
       " 'mr',\n",
       " 'pres',\n",
       " 'would',\n",
       " 'president',\n",
       " 'sen',\n",
       " 'us',\n",
       " 'united',\n",
       " 'states',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'white',\n",
       " 'house',\n",
       " 'new',\n",
       " 'york',\n",
       " 'way',\n",
       " 'people',\n",
       " 'year',\n",
       " 'years',\n",
       " 'sec',\n",
       " 'state',\n",
       " 'photo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace words with 'root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('iraqis','iraq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('iraqi','iraq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('democrats','democrat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('republicans','republican'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('israeli','israel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: x.replace('al gore', 'gore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopword_list, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = vectorizer.fit_transform(df['abstract'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40821, 43546)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02193266, 0.01377563, 0.00964389, 0.00701274, 0.00644298])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "iraq, administration, war, republican, government, security, photo, officials, military, democrat, political, congress, campaign, national, one\n",
      "\n",
      "Topic  1\n",
      "iraq, war, forces, troops, hussein, saddam, baghdad, weapons, military, government, enemy, iraqs, world, know, im\n",
      "\n",
      "Topic  2\n",
      "republican, iraq, democrat, presidential, campaign, democratic, john, senate, party, gov, kerry, mccain, election, gore, war\n",
      "\n",
      "Topic  3\n",
      "tax, billion, cuts, budget, spending, congress, cut, federal, percent, bill, plan, taxes, kerry, health, money\n",
      "\n",
      "Topic  4\n",
      "israel, palestinian, tax, prime, sharon, min, peace, arafat, plan, palestinians, security, billion, cuts, israels, ariel\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF - model generates topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(4, max_iter=300)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "iraq, war, government, security, military, troops, forces, hussein, saddam, world, weapons, baghdad, country, help, time, leaders, enemy, support, al, one, terrorists, attacks, know, strategy, iraqs\n",
      "\n",
      "Topic  1\n",
      "republican, democrat, presidential, campaign, john, senate, democratic, gov, party, election, gore, bill, political, court, vote, vice, mccain, voters, florida, national, candidates, votes, kerry, one, committee\n",
      "\n",
      "Topic  2\n",
      "administration, officials, israel, nuclear, north, military, palestinian, government, nations, security, korea, national, federal, washington, weapons, intelligence, attacks, policy, two, international, program, plan, prime, administrations, first\n",
      "\n",
      "Topic  3\n",
      "tax, billion, cuts, budget, spending, percent, congress, plan, federal, cut, bill, security, kerry, social, money, taxes, health, senate, million, next, economic, programs, deficit, government, increase\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, vectorizer.get_feature_names(), 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tfidf = TfidfVectorizer(stop_words=stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = cv_tfidf.fit_transform(df['abstract'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_2 = TruncatedSVD(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = lsa_2.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01652293, 0.01034366, 0.00198412, 0.00342278, 0.00263017])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "nan, kimpaik, letter, hayworth, enrons\n",
      "\n",
      "Topic  1\n",
      "editor, times, iraq, administration, republican\n",
      "\n",
      "Topic  2\n",
      "iraq, republican, administration, war, democrat\n",
      "\n",
      "Topic  3\n",
      "iraq, war, military, administration, weapons\n",
      "\n",
      "Topic  4\n",
      "iraq, presidential, republican, war, campaign\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_2, cv_tfidf.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model_2 = NMF(5, max_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariamyousuf/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "doc_topic = nmf_model_2.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "nan, kimpaik, hayworth, letter, enrons\n",
      "\n",
      "Topic  1\n",
      "editor, times, readers, magazine, timess\n",
      "\n",
      "Topic  2\n",
      "republican, presidential, campaign, john, gov\n",
      "\n",
      "Topic  3\n",
      "iraq, war, military, administration, weapons\n",
      "\n",
      "Topic  4\n",
      "tax, administration, congress, billion, federal\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model_2, cv_tfidf.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
