{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read cleaned abstracts and combine. Then clean some more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinton=pd.read_csv('clean_abstract_clinton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bush=pd.read_csv('clean_abstract_bush.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pres and mrs clinton hold millennium party at ...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for much of his presidency bill clintons own a...</td>\n",
       "      <td>2000-01-03 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as the first of two moving trucks turned onto ...</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it is a reality of modern campaigns that conte...</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinton pushes peace talks  president clinton ...</td>\n",
       "      <td>2000-01-05 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16436</th>\n",
       "      <td>a new gender policy council will look differen...</td>\n",
       "      <td>2021-02-16 18:27:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16437</th>\n",
       "      <td>nearly three decades after the white house est...</td>\n",
       "      <td>2021-02-16 18:27:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16438</th>\n",
       "      <td>with a following of  million and a divisive st...</td>\n",
       "      <td>2021-02-17 17:35:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16439</th>\n",
       "      <td>rush limbaugh made the gop the party of misogyny</td>\n",
       "      <td>2021-02-20 11:55:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16440</th>\n",
       "      <td>state of terror set for release in october is ...</td>\n",
       "      <td>2021-02-23 17:18:25+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16441 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      pres and mrs clinton hold millennium party at ...   \n",
       "1      for much of his presidency bill clintons own a...   \n",
       "2      as the first of two moving trucks turned onto ...   \n",
       "3      it is a reality of modern campaigns that conte...   \n",
       "4      clinton pushes peace talks  president clinton ...   \n",
       "...                                                  ...   \n",
       "16436  a new gender policy council will look differen...   \n",
       "16437  nearly three decades after the white house est...   \n",
       "16438  with a following of  million and a divisive st...   \n",
       "16439   rush limbaugh made the gop the party of misogyny   \n",
       "16440  state of terror set for release in october is ...   \n",
       "\n",
       "                            date  \n",
       "0      2000-01-02 05:00:00+00:00  \n",
       "1      2000-01-03 05:00:00+00:00  \n",
       "2      2000-01-05 05:00:00+00:00  \n",
       "3      2000-01-05 05:00:00+00:00  \n",
       "4      2000-01-05 05:00:00+00:00  \n",
       "...                          ...  \n",
       "16436  2021-02-16 18:27:12+00:00  \n",
       "16437  2021-02-16 18:27:23+00:00  \n",
       "16438  2021-02-17 17:35:38+00:00  \n",
       "16439  2021-02-20 11:55:04+00:00  \n",
       "16440  2021-02-23 17:18:25+00:00  \n",
       "\n",
       "[16441 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peter marks analysis finds that commercials ru...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the new york times the internet and political ...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>letter by mike fremont of rivers unlimited on ...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidential primary season is the most compet...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>editorial on various campaign proposals for us...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40816</th>\n",
       "      <td>republicans have criticized her tweets but dem...</td>\n",
       "      <td>2021-02-25 20:56:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40817</th>\n",
       "      <td>the disputes are reminiscent of the fight surr...</td>\n",
       "      <td>2021-02-26 00:12:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40818</th>\n",
       "      <td>most presidents leave the white house and adop...</td>\n",
       "      <td>2021-02-27 17:00:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40819</th>\n",
       "      <td>democracy an unassuming policy journal with an...</td>\n",
       "      <td>2021-02-28 22:02:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40820</th>\n",
       "      <td>despite falling from power in washington the r...</td>\n",
       "      <td>2021-03-01 22:43:35+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      peter marks analysis finds that commercials ru...   \n",
       "1      the new york times the internet and political ...   \n",
       "2      letter by mike fremont of rivers unlimited on ...   \n",
       "3      presidential primary season is the most compet...   \n",
       "4      editorial on various campaign proposals for us...   \n",
       "...                                                  ...   \n",
       "40816  republicans have criticized her tweets but dem...   \n",
       "40817  the disputes are reminiscent of the fight surr...   \n",
       "40818  most presidents leave the white house and adop...   \n",
       "40819  democracy an unassuming policy journal with an...   \n",
       "40820  despite falling from power in washington the r...   \n",
       "\n",
       "                            date  \n",
       "0      2000-01-01 05:00:00+00:00  \n",
       "1      2000-01-01 05:00:00+00:00  \n",
       "2      2000-01-02 05:00:00+00:00  \n",
       "3      2000-01-02 05:00:00+00:00  \n",
       "4      2000-01-02 05:00:00+00:00  \n",
       "...                          ...  \n",
       "40816  2021-02-25 20:56:39+00:00  \n",
       "40817  2021-02-26 00:12:38+00:00  \n",
       "40818  2021-02-27 17:00:07+00:00  \n",
       "40819  2021-02-28 22:02:50+00:00  \n",
       "40820  2021-03-01 22:43:35+00:00  \n",
       "\n",
       "[40821 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_bush.append(df_clinton, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peter marks analysis finds that commercials ru...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the new york times the internet and political ...</td>\n",
       "      <td>2000-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>letter by mike fremont of rivers unlimited on ...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidential primary season is the most compet...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>editorial on various campaign proposals for us...</td>\n",
       "      <td>2000-01-02 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57257</th>\n",
       "      <td>a new gender policy council will look differen...</td>\n",
       "      <td>2021-02-16 18:27:12+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57258</th>\n",
       "      <td>nearly three decades after the white house est...</td>\n",
       "      <td>2021-02-16 18:27:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57259</th>\n",
       "      <td>with a following of  million and a divisive st...</td>\n",
       "      <td>2021-02-17 17:35:38+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57260</th>\n",
       "      <td>rush limbaugh made the gop the party of misogyny</td>\n",
       "      <td>2021-02-20 11:55:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57261</th>\n",
       "      <td>state of terror set for release in october is ...</td>\n",
       "      <td>2021-02-23 17:18:25+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57262 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      peter marks analysis finds that commercials ru...   \n",
       "1      the new york times the internet and political ...   \n",
       "2      letter by mike fremont of rivers unlimited on ...   \n",
       "3      presidential primary season is the most compet...   \n",
       "4      editorial on various campaign proposals for us...   \n",
       "...                                                  ...   \n",
       "57257  a new gender policy council will look differen...   \n",
       "57258  nearly three decades after the white house est...   \n",
       "57259  with a following of  million and a divisive st...   \n",
       "57260   rush limbaugh made the gop the party of misogyny   \n",
       "57261  state of terror set for release in october is ...   \n",
       "\n",
       "                            date  \n",
       "0      2000-01-01 05:00:00+00:00  \n",
       "1      2000-01-01 05:00:00+00:00  \n",
       "2      2000-01-02 05:00:00+00:00  \n",
       "3      2000-01-02 05:00:00+00:00  \n",
       "4      2000-01-02 05:00:00+00:00  \n",
       "...                          ...  \n",
       "57257  2021-02-16 18:27:12+00:00  \n",
       "57258  2021-02-16 18:27:23+00:00  \n",
       "57259  2021-02-17 17:35:38+00:00  \n",
       "57260  2021-02-20 11:55:04+00:00  \n",
       "57261  2021-02-23 17:18:25+00:00  \n",
       "\n",
       "[57262 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to string needed after reading from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stopword list and add to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list.extend(['photo','monday','tuesday','wednesday','thursday','friday','saturday','sunday','rodham','im','theyre','youre','shes','who','wasnt','whom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'photo',\n",
       " 'monday',\n",
       " 'tuesday',\n",
       " 'wednesday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'saturday',\n",
       " 'sunday',\n",
       " 'rodham',\n",
       " 'im',\n",
       " 'theyre',\n",
       " 'youre',\n",
       " 'shes',\n",
       " 'who',\n",
       " 'wasnt',\n",
       " 'whom']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords from each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: ' '.join([item for item in x.split() if item not in stopword_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize each word in each abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: \" \".join([lemmatizer.lemmatize(item) for item in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words with length 1 (middle initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract']=df['abstract'].apply(lambda x: ' '.join([item for item in x.split() if len(item)>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lemma_no_stop_abstract.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
